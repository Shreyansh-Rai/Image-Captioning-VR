{"cells":[{"cell_type":"code","execution_count":116,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-04-16T13:17:09.671720Z","iopub.status.busy":"2023-04-16T13:17:09.671325Z","iopub.status.idle":"2023-04-16T13:17:09.677207Z","shell.execute_reply":"2023-04-16T13:17:09.675955Z","shell.execute_reply.started":"2023-04-16T13:17:09.671684Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T13:17:32.257795Z","iopub.status.busy":"2023-04-16T13:17:32.257057Z","iopub.status.idle":"2023-04-16T13:17:34.357597Z","shell.execute_reply":"2023-04-16T13:17:34.356499Z","shell.execute_reply.started":"2023-04-16T13:17:32.257757Z"},"trusted":true},"outputs":[],"source":["import os  # when loading file paths\n","import pandas as pd  # for lookup in annotation file\n","import spacy  # for tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence  # pad batch\n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image  # Load img\n","import torchvision.transforms as transforms\n","\n","\n","# We want to convert text -> numerical values\n","# 1. We need a Vocabulary mapping each word to a index\n","# 2. We need to setup a Pytorch dataset to load the data\n","# 3. Setup padding of every batch (all examples should be\n","#    of same seq_len and setup dataloader)\n","# Note that loading the image is very easy compared to the text!\n","\n","# Download with: python -m spacy download en\n","spacy_eng = spacy.load(\"en_core_web_sm\")\n","\n","#vocabulary is just a dictionary class that I got from stackoverflow to make life easier.\n","class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    @staticmethod\n","    def tokenizer_eng(text):\n","        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            for word in self.tokenizer_eng(sentence):\n","                if word not in frequencies:\n","                    frequencies[word] = 1\n","\n","                else:\n","                    frequencies[word] += 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        tokenized_text = self.tokenizer_eng(text)\n","\n","        return [\n","            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","            for token in tokenized_text\n","        ]\n","\n","# REFERENCE : https://www.kaggle.com/code/mdteach/torch-data-loader-flicker-8k\n","class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(captions_file)\n","        self.transform = transform\n","\n","        # Get img, caption columns\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","        # Initialize vocabulary and build vocab\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)\n","\n","\n","class MyCollate:\n","    def __init__(self, pad_idx, batch_first):\n","        self.pad_idx = pad_idx\n","        self.batch_first = batch_first\n","\n","    def __call__(self, batch):\n","        imgs = [item[0].unsqueeze(0) for item in batch]\n","        imgs = torch.cat(imgs, dim=0)\n","        targets = [item[1] for item in batch]\n","        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n","\n","        return imgs, targets\n","\n","\n","def get_loader(\n","    root_folder,\n","    annotation_file,\n","    transform,\n","    batch_size=32,\n","    num_workers=8,\n","    shuffle=True,\n","    pin_memory=True,\n","    batch_first=True\n","):\n","    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n","\n","    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","    loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=batch_size,\n","        num_workers=num_workers,\n","        shuffle=shuffle,\n","        pin_memory=pin_memory,\n","        collate_fn=MyCollate(pad_idx, batch_first),\n","    )\n","\n","    return loader, dataset\n","\n","\n","if __name__ == \"__main__\":\n","    transform = transforms.Compose(\n","        [transforms.Resize((224, 224)), transforms.ToTensor(),]\n","    )\n","\n","    loader, dataset = get_loader(\n","        \"/Users/vatsal007/Downloads/imgcap/flickr8k/images\", \"/Users/vatsal007/Downloads/imgcap/flickr8k/captions.txt\", transform=transform\n","    )\n","\n","#     for idx, (imgs, captions) in enumerate(loader):\n","#         print(imgs.shape)\n","#         print(captions.shape)"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T13:17:19.934044Z","iopub.status.busy":"2023-04-16T13:17:19.933622Z","iopub.status.idle":"2023-04-16T13:17:19.951336Z","shell.execute_reply":"2023-04-16T13:17:19.950193Z","shell.execute_reply.started":"2023-04-16T13:17:19.934000Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class CNN_Encoder(nn.Module) : \n","    def __init__(self, embedding_len, trainConvNet = False) -> None:\n","        super(CNN_Encoder, self).__init__()\n","        self.trainConvNet = False # just using a pre trained CNN to save on some time.\n","        resnet = models.resnet50(pretrained=True)  # pretrained ImageNet ResNet-101\n","\n","        # Remove linear and pool layers (since we're not doing classification)\n","        modules = list(resnet.children())[:-1]\n","        self.resnet = nn.Sequential(*modules)\n","        self.embed = nn.Linear(resnet.fc.in_features, embedding_len)\n","\n","        for p in self.resnet.parameters():\n","            p.requires_grad = False\n","        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n","        for c in list(self.resnet.children())[5:]:\n","            for p in c.parameters():\n","                p.requires_grad = self.trainConvNet\n","    \n","    def forward(self, images) :\n","        out = self.resnet(images)\n","        out = out.view(out.size(0), -1)\n","        out = self.embed(out)\n","        return out\n","    \n","class RNN(nn.Module) : \n","    def __init__(self, embedding_len, hidden_len, vocab_len, num_layers, dropout) :\n","        super(RNN,self).__init__()\n","        self.embed = nn.Embedding(vocab_len,embedding_len)\n","        self.lstm = nn.LSTM(embedding_len,hidden_len, num_layers,batch_first=True)\n","        self.linear = nn.Linear(hidden_len,vocab_len) #hidden len to vocab size which serves as indexing essentially\n","        self.dropout = nn.Dropout(dropout)\n","    def forward(self, features, captions):\n","        embeddings = self.embed(captions[:, :-1])\n","        #adding dropout post embedding layer is going to let go of certain indices and help prevent overfitting.\n","        embeddings = torch.cat((features.unsqueeze(1),embeddings),dim = 1)\n","        #Here the teacher forces all the correct captions inside as embeddings. \n","        #We feed in all the correct ground truth to the rnn and later in eval we will just feed prev cell's output\n","        #but for now this will do well since it won't accidentally end up leaerning incorrect captions.\n","        #here we want to pass the feature vector from the CNN as the first embedding to our RNN.\n","        #So we take that feature and make if of the size (1,embeding_len) unsqueeze justs makes it from \n","        #(embedding_len,) to (1,em) by adding a dimension of 1 at the 0th pos. Now we concat along row direction\n","        hiddens, _ = self.lstm(embeddings) #Pass the encoding through the linear layer to get outputs.\n","        outputs = self.linear(hiddens)\n","        return outputs\n","    \n","class Bridge(nn.Module):\n","    def __init__(self, embedding_len, hidden_len,vocab_len, num_layers, dropout) :\n","        super(Bridge,self).__init__()\n","        self.cnn_encoder = CNN_Encoder(embedding_len)\n","        self.rnn = RNN(embedding_len,hidden_len,vocab_len,num_layers,dropout)\n","    \n","    def forward(self, images, captions):\n","        features = self.cnn_encoder(images) #get image feature and pass to rnn.\n","        outputs = self.rnn(features,captions)\n","        return outputs\n","    \n","    def caption_image(self,image,vocabulary,max_len =50):\n","        captions = []\n","        #to return the captions.\n","        with torch.no_grad(): #unsqueeze for batch dimension\n","            x = self.cnn_encoder(image).unsqueeze(0) #start with input embedding as x -> image feature.\n","            states = None \n","            for _ in range(max_len):\n","                #take the hidden layer and hidden state of lstm output and hiddens will have the encoding.\n","                #pass encoding through linear layer and get the prediction and hence word.\n","                hiddens, states = self.rnn.lstm(x,states)\n","                output = self.rnn.linear(hiddens)\n","                output = output.view(x.size(0), -1)\n","                predicted = output.argmax(dim=1)\n","                #Previous prediction x is passed in the next time step to the lstm.\n","                captions.append(predicted.item())\n","                x = self.rnn.embed(predicted).unsqueeze(0)\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\" :\n","                    break\n","        return [vocabulary.itos[idx] for idx in captions]\n","\n","\n","    \n","class CNN_Encoder_Att(nn.Module) : \n","    def __init__(self, trainConvNet = False) -> None:\n","        super(CNN_Encoder_Att, self).__init__()\n","        self.trainConvNet = False # just using a pre trained CNN to save on some time.\n","        resnet = models.resnet50(pretrained=True)  # pretrained ImageNet ResNet-101\n","\n","        # Remove linear and pool layers (since we're not doing classification)\n","        modules = list(resnet.children())[:-2]\n","        self.resnet = nn.Sequential(*modules)\n","\n","        for p in self.resnet.parameters():\n","            p.requires_grad = False\n","        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n","        for c in list(self.resnet.children())[5:]:\n","            for p in c.parameters():\n","                p.requires_grad = self.trainConvNet\n","    \n","    def forward(self, images) :\n","        out = self.resnet(images)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n","        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n","        out = out.view(out.size(0), -1, out.size(-1))   # (batch_size, num_pixels, 2048)\n","        return out\n","\n","class Attention(nn.Module) :\n","    def __init__(self, enc_dim, dec_dim, att_dim):\n","        super(Attention, self).__init__()\n","        self.enc_att = nn.Linear(enc_dim, att_dim)  # linear layer to transform encoded image\n","        self.dec_att = nn.Linear(dec_dim, att_dim)  # linear layer to transform decoder's output\n","        self.full_att = nn.Linear(att_dim, 1)  # linear layer to calculate values to be softmax-ed\n","        self.relu = nn.ReLU()\n","        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n","\n","    def forward(self, features, dec_hid):\n","        encoder_att = self.enc_att(features)  # (batch_size, num_pixels, attention_dim)\n","        decoder_att = self.dec_att(dec_hid)  # (batch_size, attention_dim)\n","        att = self.full_att(self.relu(encoder_att + decoder_att.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n","        alpha = self.softmax(att)  # (batch_size, num_pixels)\n","        att_weights = (features * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, enc_dim)\n","\n","        return alpha, att_weights\n","    \n","class RNN_Att(nn.Module):\n","    def __init__(self, att_dim, embedding_len, dec_dim, vocab_len, enc_dim=2048, dropout=0.5):\n","        super(RNN_Att, self).__init__()\n","\n","        self.vocab_len = vocab_len\n","\n","        self.attention = Attention(enc_dim, dec_dim, att_dim)       # attention network\n","        self.embed = nn.Embedding(vocab_len, embedding_len)  # embedding layer\n","        self.dropout = nn.Dropout(dropout)\n","        self.lstm_cell = nn.LSTMCell(embedding_len + enc_dim, dec_dim, bias=True)  # decoding LSTMCell\n","        self.init_h = nn.Linear(enc_dim, dec_dim)  # linear layer to find initial hidden state of LSTMCell\n","        self.init_c = nn.Linear(enc_dim, dec_dim)  # linear layer to find initial cell state of LSTMCell\n","\n","        self.fc = nn.Linear(dec_dim, vocab_len)  # linear layer to find scores over vocabulary\n","\n","    def forward(self, features, captions):\n","        embeddings = self.embed(captions)\n","        h, c = self.init_hidden_state(features)\n","\n","        cap_len = len(captions[0]) - 1\n","        batch_size = captions.size(0)\n","        num_features = features.size(1)\n","\n","        preds = torch.zeros(batch_size, cap_len, self.vocab_len).to(device)\n","        alphas = torch.zeros(batch_size, cap_len, num_features).to(device)\n","\n","        for cap in range(cap_len):\n","            alpha, context = self.attention(features, h)\n","            # print(np.shape(embeddings[:, cap]), np.shape(context))\n","            lstm_in = torch.cat([embeddings[:, cap], context], dim=1)\n","            h, c = self.lstm_cell(lstm_in, (h, c))\n","            out = self.fc(self.dropout(h))\n","\n","            preds[:, cap] = out\n","            alphas[:, cap] = alpha\n","\n","        return preds, alphas\n","\n","    def init_hidden_state(self, features):\n","        mean_features = features.mean(dim=1)\n","        h = self.init_h(mean_features)  # (batch_size, decoder_dim)\n","        c = self.init_c(mean_features)\n","        return h, c\n","    \n","class Bridge_Att(nn.Module):\n","    def __init__(self, embedding_len, vocab_len, att_dim, enc_dim, dec_dim) :\n","        super(Bridge_Att,self).__init__()\n","        self.cnn_encoder = CNN_Encoder_Att()\n","        self.rnn = RNN_Att(att_dim, embedding_len, dec_dim, vocab_len, enc_dim)\n","\n","    def forward(self, images, captions):\n","        features = self.cnn_encoder(images) #get image feature and pass to rnn.\n","        outputs = self.rnn(features, captions)\n","        return outputs\n","    \n","    def caption_image(self,image, vocabulary, max_len=50):\n","        with torch.no_grad():\n","            features = self.cnn_encoder(image)\n","\n","            batch_size = features.size(0)\n","            h, c = self.rnn.init_hidden_state(features)\n","\n","            alphas = []\n","            captions = []\n","\n","            #first input\n","            word = torch.tensor(vocabulary.stoi['<SOS>']).view(1,-1).to(device)\n","            embeddings = self.rnn.embed(word)\n","\n","            for _ in range(max_len):\n","                alpha, context = self.rnn.attention(features, h)\n","                alphas.append(alpha.cpu().detach().numpy())\n","                \n","                lstm_in = torch.cat([embeddings[:, 0], context], dim=1)\n","                h, c = self.rnn.lstm_cell(lstm_in, (h, c))\n","                out = self.rnn.fc(self.rnn.dropout(h))\n","\n","                out = out.view(batch_size, -1)\n","                predicted = out.argmax(dim=1)\n","\n","                captions.append(predicted.item())\n","                #Previous prediction embeddings is passed in the next time step to the lstm.\n","                embeddings = self.rnn.embed(predicted.unsqueeze(0))\n","\n","                if vocabulary.itos[predicted.item()] == \"<EOS>\" :\n","                    break\n","            return [vocabulary.itos[idx] for idx in captions], alphas\n","\n"]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[],"source":["refsent1 = [\"A girl and her horse stand by a fire .\",\"A girl holding a horse 's lead behind a fire .\",\"A man , and girl and two horses are near a contained fire .\",\"Two people and two horses watching a fire .\"]\n","refsent2 = [\"A man fishes by a tree in the morning mist .\",\"A man fishes under a large tree .\",\"A man fishing near a large tree .\",\"A man is fishing in a foggy lake .\"]\n","refsent3 = [\"a kayaker kayaks through the water .\",\"A person kayaking in the ocean .\",\"A person kayaks in the middle of the ocean on a grey day .\",\"A person rows a boat over a large body of water .\",\"person in a boat with a paddle in hand .\"]\n","refsent4 = [\"A man in a red jacket is sitting on a bench whilst cooking a meal .\",\"A man sits on a bench .\",\"A man is sitting on a bench , cooking some food .\",\"A man wearing a red jacket is sitting on a wooden bench and is cooking something in a small pot .\",\"a man wearing a red jacket sitting on a bench next to various camping items\"]\n","refsent5 = [\"A couple stands close at the water 's edge .\",\"The two people stand by a body of water and in front of bushes in fall .\",\"Two people hold each other near a pond .\",\"Two people stand by the water .\",\"Two people stand together on the edge of the water on the grass .\"]"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T13:18:49.313571Z","iopub.status.busy":"2023-04-16T13:18:49.312740Z","iopub.status.idle":"2023-04-16T13:18:49.327447Z","shell.execute_reply":"2023-04-16T13:18:49.326163Z","shell.execute_reply.started":"2023-04-16T13:18:49.313528Z"},"trusted":true},"outputs":[],"source":["printouts = []\n","def print_examples(model, device, dataset):\n","    printouts.clear()\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(224),\n","            # transforms.RandomCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n","        ]\n","    )\n","\n","    model.eval()\n","\n","    if(type(model)==Bridge_Att):\n","        test_img1 = transform(Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/10815824_2997e03d76.jpg\").convert(\"RGB\")).unsqueeze(\n","            0\n","        )\n","        print(\"Example 1 CORRECT: A girl and her horse stand by a fire\")\n","        out, _ = model.caption_image(test_img1.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(printouts)\n","        print(\n","            \"Example 1 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img2 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/17273391_55cfc7d3d4.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 2 CORRECT: A man fishes under a large tree\")\n","        out, _ = model.caption_image(test_img2.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(printouts)\n","        print(\n","            \"Example 2 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img3 = transform(Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/19212715_20476497a3.jpg\").convert(\"RGB\")).unsqueeze(\n","            0\n","        )\n","        print(\"Example 3 CORRECT: A person kayaking in the ocean\")\n","        out, _ = model.caption_image(test_img3.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 3 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img4 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/35506150_cbdb630f4f.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 4 CORRECT: A man is sitting on a bench , cooking some food .\")\n","        out, _ = model.caption_image(test_img4.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 4 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img5 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/3637013_c675de7705.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 5 CORRECT: Two people stand together on the edge of the water on the grass\")\n","        out, _ = model.caption_image(test_img5.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 5 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","    else:\n","        test_img1 = transform(Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/10815824_2997e03d76.jpg\").convert(\"RGB\")).unsqueeze(\n","            0\n","        )\n","        print(\"Example 1 CORRECT: A girl and her horse stand by a fire\")\n","        out = model.caption_image(test_img1.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(printouts)\n","        print(\n","            \"Example 1 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img2 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/17273391_55cfc7d3d4.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 2 CORRECT: A man fishes under a large tree\")\n","        out = model.caption_image(test_img2.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(printouts)\n","        print(\n","            \"Example 2 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img3 = transform(Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/19212715_20476497a3.jpg\").convert(\"RGB\")).unsqueeze(\n","            0\n","        )\n","        print(\"Example 3 CORRECT: A person kayaking in the ocean\")\n","        out = model.caption_image(test_img3.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 3 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img4 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/35506150_cbdb630f4f.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 4 CORRECT: A man is sitting on a bench , cooking some food .\")\n","        out = model.caption_image(test_img4.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 4 OUTPUT: \"\n","            + \" \".join(out)\n","        )\n","        test_img5 = transform(\n","            Image.open(\"/Users/vatsal007/Downloads/imgcap/flickr8k/ti/3637013_c675de7705.jpg\").convert(\"RGB\")\n","        ).unsqueeze(0)\n","        print(\"Example 5 CORRECT: Two people stand together on the edge of the water on the grass\")\n","        out = model.caption_image(test_img5.to(device), dataset.vocab)\n","        printouts.append(out)\n","        print(\n","            \"Example 5 OUTPUT: \"\n","            + \" \".join(out)\n","        )"]},{"cell_type":"code","execution_count":121,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T13:19:47.409194Z","iopub.status.busy":"2023-04-16T13:19:47.408800Z","iopub.status.idle":"2023-04-16T13:26:39.199272Z","shell.execute_reply":"2023-04-16T13:26:39.197408Z","shell.execute_reply.started":"2023-04-16T13:19:47.409159Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"name":"stderr","output_type":"stream","text":["/Users/vatsal007/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/Users/vatsal007/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"name":"stdout","output_type":"stream","text":["!!!!Loading checkpoint!!!!\n","Example 1 CORRECT: A girl and her horse stand by a fire\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>']]\n","Example 1 OUTPUT: <SOS> a girl and her horse stand by a fire . <EOS>\n","Example 2 CORRECT: A man fishes under a large tree\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>'], ['<SOS>', 'a', 'man', 'fishes', 'under', 'a', 'large', 'tree', '.', '<EOS>']]\n","Example 2 OUTPUT: <SOS> a man fishes under a large tree . <EOS>\n","Example 3 CORRECT: A person kayaking in the ocean\n","Example 3 OUTPUT: <SOS> a person kayaks in the middle of the ocean on a grey day . <EOS>\n","Example 4 CORRECT: A man is sitting on a bench , cooking some food .\n","Example 4 OUTPUT: <SOS> a man is sitting on a bench and reading a book . <EOS>\n","Example 5 CORRECT: Two people stand together on the edge of the water on the grass\n","Example 5 OUTPUT: <SOS> two people stand by the water . <EOS>\n","Done\n"]},{"name":"stderr","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","  File \"/Users/vatsal007/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/spawn.py\", line 116, in spawn_main\n","    exitcode = _main(fd, parent_sentinel)\n","  File \"/Users/vatsal007/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/spawn.py\", line 126, in _main\n","    self = reduction.pickle.load(from_parent)\n","AttributeError: Can't get attribute 'FlickrDataset' on <module '__main__' (built-in)>\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[121], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m             optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 105\u001b[0m     train()\n","Cell \u001b[0;32mIn[121], line 86\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     save_checkpoint(checkpoint)\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m idx, (imgs, captions) \u001b[39min\u001b[39;00m tqdm(\n\u001b[0;32m---> 86\u001b[0m     \u001b[39menumerate\u001b[39;49m(train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     87\u001b[0m ):\n\u001b[1;32m     88\u001b[0m     \u001b[39m# print(np.shape(imgs))\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     90\u001b[0m     captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(device)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:442\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 388\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1043\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1036\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1044\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n","File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n","    print(\"!!!!Saving checkpoint!!!!\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"!!!!Loading checkpoint!!!!\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step\n","\n","def train():\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(226),\n","            transforms.RandomCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n","        ]\n","    )\n","\n","    train_loader, dataset = get_loader(\n","        root_folder=\"/Users/vatsal007/Downloads/imgcap/flickr8k/images\",\n","        annotation_file=\"/Users/vatsal007/Downloads/imgcap/flickr8k/captions.txt\",\n","        transform=transform,\n","        num_workers=2,\n","        batch_first=True\n","    )\n","\n","    torch.backends.cudnn.benchmark = True\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    load_model = True\n","    save_model = False\n","    trainConvnet = False\n","\n","    # Hyperparameters\n","    embedding_len = 400\n","    hidden_len = 512\n","    vocab_len = len(dataset.vocab)\n","    num_layers = 2\n","    learning_rate = 3e-4\n","    num_epochs = 200\n","\n","    # for tensorboard\n","    writer = SummaryWriter(\"runs/flickr\")\n","    step = 0\n","\n","    # initialize model, loss etc\n","    model = Bridge(embedding_len, hidden_len, vocab_len, num_layers, 0.5).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Only finetune the CNN\n","    for name, param in model.cnn_encoder.resnet.named_parameters():\n","        if \"fc.weight\" in name or \"fc.bias\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = trainConvnet\n","\n","    if load_model:\n","        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\",map_location=device), model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        print_examples(model, device, dataset)\n","\n","        if save_model:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","                \"step\": step,\n","            }\n","            save_checkpoint(checkpoint)\n","        print(\"Done\")\n","        for idx, (imgs, captions) in tqdm(\n","            enumerate(train_loader), total=len(train_loader), leave=False\n","        ):\n","            # print(np.shape(imgs))\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs = model(imgs, captions)\n","            loss = criterion(\n","                outputs.view(-1, outputs.shape[2]), captions.view(-1)\n","            )\n","\n","            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()\n","\n","if __name__ == \"__main__\":\n","    train()"]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2023-04-16T13:26:42.970220Z","iopub.status.busy":"2023-04-16T13:26:42.969829Z","iopub.status.idle":"2023-04-16T13:26:43.575334Z","shell.execute_reply":"2023-04-16T13:26:43.574244Z","shell.execute_reply.started":"2023-04-16T13:26:42.970182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["!!!!Loading checkpoint!!!!\n","<class '__main__.Bridge'>\n","Example 1 CORRECT: A girl and her horse stand by a fire\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>']]\n","Example 1 OUTPUT: <SOS> a girl and her horse stand by a fire . <EOS>\n","Example 2 CORRECT: A man fishes under a large tree\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>'], ['<SOS>', 'a', 'man', 'fishes', 'under', 'a', 'large', 'tree', '.', '<EOS>']]\n","Example 2 OUTPUT: <SOS> a man fishes under a large tree . <EOS>\n","Example 3 CORRECT: A person kayaking in the ocean\n","Example 3 OUTPUT: <SOS> a person kayaks in the middle of the ocean on a grey day . <EOS>\n","Example 4 CORRECT: A man is sitting on a bench , cooking some food .\n","Example 4 OUTPUT: <SOS> a man is sitting on a bench and reading a book . <EOS>\n","Example 5 CORRECT: Two people stand together on the edge of the water on the grass\n","Example 5 OUTPUT: <SOS> two people stand by the water . <EOS>\n"]}],"source":["embedding_len = 400\n","hidden_len = 512\n","vocab_len = len(dataset.vocab)\n","num_layers = 2\n","learning_rate = 3e-4\n","model = Bridge(embedding_len, hidden_len, vocab_len, num_layers, 0.5).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","load_checkpoint(torch.load(\"my_checkpoint.pth.tar\",map_location=device), model, optimizer)\n","\n","print(type(model))\n","print_examples(model, device, dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.tensorboard import SummaryWriter\n","def save_checkpoint(state, filename=\"my_checkpoint_att.pth.tar\"):\n","    print(\"!!!!Saving checkpoint!!!!\")\n","    torch.save(state, filename)\n","\n","\n","def load_checkpoint(checkpoint, model, optimizer):\n","    print(\"!!!!Loading checkpoint!!!!\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])\n","    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n","    step = checkpoint[\"step\"]\n","    return step\n","\n","def train():\n","    transform = transforms.Compose(\n","        [\n","            transforms.Resize(226),\n","            transforms.RandomCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225)),\n","        ]\n","    )\n","\n","    train_loader, dataset = get_loader(\n","        root_folder=\"/Users/vatsal007/Downloads/imgcap/flickr8k/images\",\n","        annotation_file=\"/Users/vatsal007/Downloads/imgcap/flickr8k/captions.txt\",\n","        transform=transform,\n","        num_workers=2,\n","        batch_first=True\n","    )\n","\n","    torch.backends.cudnn.benchmark = True\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(device)\n","    load_model = True\n","    save_model = False\n","    trainConvnet = False\n","\n","    # Hyperparameters\n","    embedding_len = 256\n","    vocab_len = len(dataset.vocab)\n","    att_dim=256\n","    enc_dim=2048\n","    dec_dim=512\n","    learning_rate = 3e-4\n","    num_epochs = 200\n","\n","    # for tensorboard\n","    writer = SummaryWriter(\"runs/flickr\")\n","    step = 0\n","\n","    # initialize model, loss etc\n","    model = Bridge_Att(embedding_len, vocab_len, att_dim, enc_dim, dec_dim).to(device)\n","    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # Only finetune the CNN\n","    for name, param in model.cnn_encoder.resnet.named_parameters():\n","        if \"fc.weight\" in name or \"fc.bias\" in name:\n","            param.requires_grad = True\n","        else:\n","            param.requires_grad = trainConvnet\n","\n","    if load_model:\n","        step = load_checkpoint(torch.load(\"my_checkpoint_att.pth.tar\",map_location=device), model, optimizer)\n","\n","    model.train()\n","\n","    for epoch in range(num_epochs):\n","        # Uncomment the line below to see a couple of test cases\n","        print_examples(model, device, dataset)\n","\n","        if save_model:\n","            checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"optimizer\": optimizer.state_dict(),\n","                \"step\": step,\n","            }\n","            save_checkpoint(checkpoint)\n","        print(\"Done\")\n","        for idx, (imgs, captions) in tqdm(\n","            enumerate(train_loader), total=len(train_loader), leave=False\n","        ):\n","            # print(np.shape(imgs))\n","            imgs = imgs.to(device)\n","            captions = captions.to(device)\n","\n","            outputs, attentions = model(imgs, captions)\n","            loss = criterion(\n","                outputs.view(-1, vocab_len), captions[:,1:].reshape(-1)\n","            )\n","\n","            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n","            step += 1\n","\n","            optimizer.zero_grad()\n","            loss.backward(loss)\n","            optimizer.step()\n","\n","if __name__ == \"__main__\":\n","    train()"]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["!!!!Loading checkpoint!!!!\n","Example 1 CORRECT: A girl and her horse stand by a fire\n","[['a', 'girl', 'in', 'a', 'brown', 'shirt', 'is', 'playing', 'with', 'a', 'fire', '.', '<EOS>']]\n","Example 1 OUTPUT: a girl in a brown shirt is playing with a fire . <EOS>\n","Example 2 CORRECT: A man fishes under a large tree\n","[['a', 'girl', 'in', 'a', 'brown', 'shirt', 'is', 'playing', 'with', 'a', 'fire', '.', '<EOS>'], ['a', 'man', 'fishes', 'under', 'a', 'large', 'tree', '.', '<EOS>']]\n","Example 2 OUTPUT: a man fishes under a large tree . <EOS>\n","Example 3 CORRECT: A person kayaking in the ocean\n","Example 3 OUTPUT: a person kayaks in the middle of the ocean . <EOS>\n","Example 4 CORRECT: A man is sitting on a bench , cooking some food .\n","Example 4 OUTPUT: a man wearing a red jacket is sitting on a bench . <EOS>\n","Example 5 CORRECT: Two people stand together on the edge of the water on the grass\n","Example 5 OUTPUT: a man is rowing a canoe in a river . <EOS>\n"]}],"source":["embedding_len = 256\n","vocab_len = len(dataset.vocab)\n","att_dim=256\n","enc_dim=2048\n","dec_dim=512\n","learning_rate = 3e-4\n","model_att = Bridge_Att(embedding_len, vocab_len, att_dim, enc_dim, dec_dim).to(device)\n","optimizer_att = optim.Adam(model_att.parameters(), lr=learning_rate)\n","load_checkpoint(torch.load(\"my_checkpoint_att.pth.tar\",map_location=device), model_att, optimizer_att)\n","\n","\n","print_examples(model_att, device, dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Bleu4 scores and Meteor Scores"]},{"cell_type":"code","execution_count":125,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/vatsal007/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Error loading corpora: Package 'corpora' not found in\n","[nltk_data]     index\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/vatsal007/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     /Users/vatsal007/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /Users/vatsal007/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]}],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('corpora')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate import meteor_score"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Without Attention"]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: A girl and her horse stand by a fire\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>']]\n","Example 1 OUTPUT: <SOS> a girl and her horse stand by a fire . <EOS>\n","Example 2 CORRECT: A man fishes under a large tree\n","[['<SOS>', 'a', 'girl', 'and', 'her', 'horse', 'stand', 'by', 'a', 'fire', '.', '<EOS>'], ['<SOS>', 'a', 'man', 'fishes', 'under', 'a', 'large', 'tree', '.', '<EOS>']]\n","Example 2 OUTPUT: <SOS> a man fishes under a large tree . <EOS>\n","Example 3 CORRECT: A person kayaking in the ocean\n","Example 3 OUTPUT: <SOS> a person kayaks in the middle of the ocean on a grey day . <EOS>\n","Example 4 CORRECT: A man is sitting on a bench , cooking some food .\n","Example 4 OUTPUT: <SOS> a man is sitting on a bench and reading a book . <EOS>\n","Example 5 CORRECT: Two people stand together on the edge of the water on the grass\n","Example 5 OUTPUT: <SOS> two people stand by the water . <EOS>\n"]}],"source":["print_examples(model, device, dataset)"]},{"cell_type":"code","execution_count":131,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["example:  1\n","BLEU-4 score: 0.8070557274927982\n","METEOR score: 0.9799019607843138 \n","\n","\n","example:  2\n","BLEU-4 score: 0.7598356856515925\n","METEOR score: 0.9746570121951219 \n","\n","\n","example:  3\n","BLEU-4 score: 0.861173529963367\n","METEOR score: 0.985735843633228 \n","\n","\n","example:  4\n","BLEU-4 score: 0.4574563333993253\n","METEOR score: 0.6228465412138882 \n","\n","\n","example:  5\n","BLEU-4 score: 0.7259795291154771\n","METEOR score: 0.9708049886621315 \n","\n","\n"]}],"source":["candidate_sentencelist = printouts\n","reference_sentenceslist = [refsent1,refsent2,refsent3,refsent4,refsent5]\n","\n","for i in range(5):\n","    print(\"example: \",i+1)\n","    candidate_sentence = candidate_sentencelist[i]\n","    reference_sentences = reference_sentenceslist[i]\n","\n","    candidate_tokens = candidate_sentence\n","\n","    reference_tokens = []\n","    for sentence in reference_sentences:\n","        reference_tokens.append(nltk.word_tokenize(sentence.lower()))\n","\n","\n","\n","\n","    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n","    print(\"BLEU-4 score:\", bleu_score)\n","\n","    score = meteor_score.meteor_score(reference_tokens, candidate_tokens)\n","    print(\"METEOR score:\", score,\"\\n\\n\")"]},{"cell_type":"code","execution_count":132,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Example 1 CORRECT: A girl and her horse stand by a fire\n","[['a', 'girl', 'in', 'a', 'brown', 'shirt', 'is', 'playing', 'with', 'a', 'fire', '.', '<EOS>']]\n","Example 1 OUTPUT: a girl in a brown shirt is playing with a fire . <EOS>\n","Example 2 CORRECT: A man fishes under a large tree\n","[['a', 'girl', 'in', 'a', 'brown', 'shirt', 'is', 'playing', 'with', 'a', 'fire', '.', '<EOS>'], ['a', 'man', 'fishes', 'under', 'a', 'large', 'tree', '.', '<EOS>']]\n","Example 2 OUTPUT: a man fishes under a large tree . <EOS>\n","Example 3 CORRECT: A person kayaking in the ocean\n","Example 3 OUTPUT: a person kayaks in the middle of the ocean . <EOS>\n","Example 4 CORRECT: A man is sitting on a bench , cooking some food .\n","Example 4 OUTPUT: a man wearing a red jacket is sitting on a bench . <EOS>\n","Example 5 CORRECT: Two people stand together on the edge of the water on the grass\n","Example 5 OUTPUT: a man is rowing a canoe in a river . <EOS>\n"]}],"source":["print_examples(model_att, device, dataset)"]},{"cell_type":"code","execution_count":133,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["example:  1\n","BLEU-4 score: 3.6192421858902592e-78\n","METEOR score: 0.5022321428571428 \n","\n","\n","example:  2\n","BLEU-4 score: 0.8633400213704505\n","METEOR score: 0.9866898148148149 \n","\n","\n","example:  3\n","BLEU-4 score: 0.8593887047640296\n","METEOR score: 0.9349145063430778 \n","\n","\n","example:  4\n","BLEU-4 score: 0.912167909070388\n","METEOR score: 0.8848012889366274 \n","\n","\n","example:  5\n","BLEU-4 score: 1.3165594234639305e-231\n","METEOR score: 0.10869565217391303 \n","\n","\n"]}],"source":["candidate_sentencelist = printouts\n","reference_sentenceslist = [refsent1,refsent2,refsent3,refsent4,refsent5]\n","\n","for i in range(5):\n","    print(\"example: \",i+1)\n","    candidate_sentence = candidate_sentencelist[i]\n","    reference_sentences = reference_sentenceslist[i]\n","\n","    candidate_tokens = candidate_sentence\n","\n","    reference_tokens = []\n","    for sentence in reference_sentences:\n","        reference_tokens.append(nltk.word_tokenize(sentence.lower()))\n","\n","\n","\n","\n","    bleu_score = sentence_bleu(reference_tokens, candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25))\n","    print(\"BLEU-4 score:\", bleu_score)\n","\n","    score = meteor_score.meteor_score(reference_tokens, candidate_tokens)\n","    print(\"METEOR score:\", score,\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import copy\n","def show_image(img, title=None):\n","\n","    img = img.numpy().transpose((1, 2, 0))\n","\n","    plt.imshow(img)\n","    if title is not None:\n","        plt.title(title)\n","    plt.pause(0.001)  # pause a bit so that plots are updated\n","\n","def get_captions_att(image):\n","    #generate the caption\n","    model_att.eval()\n","    with torch.no_grad():\n","        caps, alphas = model_att.caption_image(image.to(device), dataset.vocab)\n","        caption = ' '.join(caps)\n","        show_image(image[0],title=caption)\n","    return caps,alphas\n","\n","def get_captions(image):\n","    #generate the caption\n","    model.eval()\n","    with torch.no_grad():\n","        caps = model.caption_image(image.to(device), dataset.vocab)\n","        caption = ' '.join(caps)\n","        show_image(image[0],title=caption)\n","    return caps\n","\n","#Show attention\n","def plot_attention(img, result, attention_plot):\n","\n","    img = img.numpy().transpose((1, 2, 0))\n","    temp_image = img\n","\n","    fig = plt.figure(figsize=(15, 15))\n","\n","    len_result = len(result)\n","    for l in range(len_result):\n","        temp_att = attention_plot[l].reshape(7,7)\n","\n","        ax = fig.add_subplot(len_result//2,len_result//2, l+1)\n","        ax.set_title(result[l])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.7, extent=img.get_extent())\n","\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_img1 = transform(Image.open(\"/home/sony-r301/ImageCaptioning/ImageCaptioning/test_examples/dog.jpg\").convert(\"RGB\"))\n","img1 = copy.deepcopy(test_img1)\n","capsatt, alphas = get_captions_att(test_img1.unsqueeze(0))\n","caps = get_captions(test_img1.unsqueeze(0))\n","plot_attention(img1, capsatt, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_img1 = transform(Image.open(\"/home/sony-r301/ImageCaptioning/ImageCaptioning/test_examples/dog.jpg\").convert(\"RGB\"))\n","img1 = copy.deepcopy(test_img1)\n","capsatt, alphas = get_captions_att(test_img1.unsqueeze(0))\n","caps = get_captions(test_img1.unsqueeze(0))\n","plot_attention(img1, capsatt, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_img1 = transform(Image.open(\"/home/sony-r301/ImageCaptioning/ImageCaptioning/test_examples/dog.jpg\").convert(\"RGB\"))\n","img1 = copy.deepcopy(test_img1)\n","capsatt, alphas = get_captions_att(test_img1.unsqueeze(0))\n","caps = get_captions(test_img1.unsqueeze(0))\n","plot_attention(img1, capsatt, alphas)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_img1 = transform(Image.open(\"/home/sony-r301/ImageCaptioning/ImageCaptioning/test_examples/dog.jpg\").convert(\"RGB\"))\n","img1 = copy.deepcopy(test_img1)\n","capsatt, alphas = get_captions_att(test_img1.unsqueeze(0))\n","caps = get_captions(test_img1.unsqueeze(0))\n","plot_attention(img1, capsatt, alphas)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
